{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get article titles and teaser text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html  #get all the html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#testing output, see what to get\n",
    "#print(soup.prettify())\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#The below will get you *all* the titles and articles. The final requirements only have 1\n",
    "# titles=[]\n",
    "# teaser_texts=[]\n",
    "# divs = soup.find_all('div')\n",
    "# for div in divs:\n",
    "#     #print(div['class'][0])  #this will get you content_title and article_teaser_body\n",
    "#     if div['class'][0]=='content_title':\n",
    "#         #print(div.text)\n",
    "#         titles.append(div.text)\n",
    "#     if div['class'][0]=='article_teaser_body':\n",
    "#         #print(div.text)\n",
    "#         teaser_texts.append(div.text)\n",
    "#print(titles[0])\n",
    "#print(teaser_texts[0])\n",
    "\n",
    "title = soup.find('div', class_='content_title').text\n",
    "teaser_text = soup.find('div', class_='article_teaser_body').text\n",
    "print(teaser_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA Invites Public to Share Excitement of Mars 2020 Perseverance Rover Launch\n",
    "# There are lots of ways to participate in the historic event, which is targeted for July 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get featured image url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iurl = 'https://spaceimages-mars.com/'\n",
    "browser.visit(iurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihtml = browser.html\n",
    "isoup = BeautifulSoup(ihtml, 'html.parser')\n",
    "\n",
    "#checkout html output, looking for featured image \n",
    "#print(isoup.prettify())\n",
    "\n",
    "# looking for this src:\n",
    "# <img class=\"headerimage fade-in\" src=\"image/featured/mars1.jpg\">\n",
    "\n",
    "imgs = isoup.find_all('img')\n",
    "for img in imgs:\n",
    "    #print(img) #testing message\n",
    "    try:\n",
    "        if img['class'][0]=='headerimage':\n",
    "            #print(img['src'])  #testing message\n",
    "            featured_image_file = img['src']\n",
    "            break   #header image will be towards the top, once found don't need to continue the loop\n",
    "    except:  #if there's no class, continue to the next one\n",
    "        continue\n",
    "\n",
    "#concat the url to the image location\n",
    "featured_image_url = iurl+featured_image_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the url value\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Mars facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turl = 'https://galaxyfacts-mars.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(turl)\n",
    "tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_earth_df = tables[0]\n",
    "mars_facts_df = tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_df.rename(columns={0:\"FACT_DESC\",1:\"FACT_VALUE\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_earth_df.rename(columns={0:\"ATTRIBUTE\",1:\"MARS\",2:\"EARTH\"}, inplace=True)\n",
    "mars_earth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the 1st row which is more of a table header\n",
    "mars_earth_df =mars_earth_df.drop(0)  \n",
    "mars_earth_df  #not bothering to reindex because not keeping it in the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push dfs to html files\n",
    "mars_earth_df.to_html('mars_earth_comp.html', index=False)\n",
    "mars_facts_df.to_html('mars_facts.html', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hemisphere info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurl = 'https://marshemispheres.com/'\n",
    "browser.visit(hurl)\n",
    "hemisphere_image_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html  #get all the html\n",
    "hsoup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get the list of 4 headers from main page and then loop through each.\n",
    "list_of_subpages = []\n",
    "\n",
    "a_itemlinks = hsoup.find_all('a', class_='itemLink')\n",
    "\n",
    "for alink in a_itemlinks:\n",
    "    #print(alink['href']) #testing output\n",
    "    cur_href = alink['href']\n",
    "    if 'html' in cur_href:\n",
    "        #print('it does') #testing output\n",
    "        if cur_href not in list_of_subpages:\n",
    "            #print('add it!') #testing output\n",
    "            list_of_subpages.append(cur_href)\n",
    "\n",
    "list_of_subpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in list_of_subpages:\n",
    "    sub_url=hurl+page\n",
    "    #print(sub_url) #testing output\n",
    "    \n",
    "    browser.visit(sub_url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #print(soup.prettify()) #testing output\n",
    "    \n",
    "    #looking for this <img class=\"wide-image\" src=\"images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg\"/>\n",
    "    img = soup.find('img', class_='wide-image')\n",
    "    title=soup.find('h2', class_='title').text.split(\" Enhanced\")[0]\n",
    "    #print(hurl+img['src'])  #testing output\n",
    "    hemis_dict = {}\n",
    "    hemis_dict['title'] = title\n",
    "    hemis_dict['img_url'] = hurl+img['src']\n",
    "    #print(hemis_dict)  #testing output\n",
    "    \n",
    "    hemisphere_image_urls.append(hemis_dict)\n",
    "\n",
    "    #another way to do this would've been to start in landing page, use a click to get to each subpage and then would use the below click to go back\n",
    "    #browser.links.find_by_partial_text('Back').click()\n",
    "    #break #testing break. got code working for 1st page then removed this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dictionary is properly populated\n",
    "hemisphere_image_urls\n",
    "\n",
    "\n",
    "#testing output, see what to get\n",
    "#print(hsoup.prettify())\n",
    "\n",
    "#print(a_itemlinks.find('img'))\n",
    "#img_info=a_itemlinks.find('img')\n",
    "\n",
    "#this is getting the thumbnail off the main page\n",
    "# hemis_dict = {}\n",
    "# hemis_dict['title'] = img_info['alt'].split(\" Enhanced\")[0]\n",
    "# hemis_dict['img_url'] = hurl+img_info['src']\n",
    "# img_info['alt'].split(\" Enhanced\")[0]  #title!  yes, there are other ways to get this. could take if off 1st page too\n",
    "# hurl+img_info['src']\n",
    "#browser.links.find_by_partial_text('Back').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1):\n",
    "    #browser.links.find_by_partial_text('Hemisphere').click()  #partial text needs to be a variable from loop\n",
    "    browser.links.find_by_partial_text('Schiaparelli').click() \n",
    "    lhtml = browser.html  #get all the html\n",
    "    lsoup = BeautifulSoup(lhtml, 'html.parser')\n",
    "    #testing output, see what to get\n",
    "    print(lsoup.prettify())\n",
    "    print('--------------------------------------------------------------')\n",
    "    a_itemlinks = lsoup.find('a', class_='itemLink')\n",
    "    #h3s = hsoup.find_all('h3')\n",
    "    print(a_itemlinks.find('img'))\n",
    "    img_info=a_itemlinks.find('img')\n",
    "    print('fucking'+img_info['alt'])\n",
    "    hemis_dict = {}\n",
    "    hemis_dict['title'] = img_info['alt'].split(\" Enhanced\")[0]\n",
    "    hemis_dict['img_url'] = hurl+img_info['src']\n",
    "    #img_info['alt'].split(\" Enhanced\")[0]  #title!  yes, there are other ways to get this. could take if off 1st page too\n",
    "    #hurl+img_info['src']\n",
    "    hemisphere_image_urls.append(hemis_dict)\n",
    "    \n",
    "    \n",
    "    #browser.links.find_by_partial_text('Back').click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemis_dict\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
